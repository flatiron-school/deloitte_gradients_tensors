{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradients and Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a certain amount of mathematics we'll need before proceeding. This lecture will introduce you to two mathematical notions that are central to machine learning: **gradients** and **tensors**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient is a notion from calculus, and it is a generalized notion of a derivative into higher-dimensional spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivatives in One Dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you first learn calculus, you generally start with *differential* calculus, and you always start with functions of a single variable. Thus you learn derivative shortcut rules such as:\n",
    "\n",
    "$\\frac{d}{dx}[x^2] = 2x$\n",
    "\n",
    "$\\frac{d}{dx}[sin(x)] = cos(x)$\n",
    "\n",
    "$\\frac{d}{dx}[5^x] = 5^xln(5)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there is the Newtonian notation as well:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f'(x) = 2x$ for $f(x) = x^2$\n",
    "\n",
    "$f'(x) = cos(x)$ for $f(x) = sin(x)$\n",
    "\n",
    "$f'(x) = 5^xln(5)$ for $f(x) = 5^x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivatives of Functions of Multiple Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind that a derivative tells us the *rate of change* of a function for any value of its independent variables. To say e.g. that $2x$ is the derivative of $x^2$ is to say that, for any value of $x$, the rate of change, or *slope*, of the function $x^2$ has a value of $2x$. At $x=0$, the slope is $2\\times 0 = 0$; at $x=1$, the slope is $2\\times 1 = 2$; etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.linspace(-3, 3, 13)\n",
    "y = X**2\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "ax.plot(X, y)\n",
    "ax.plot(X, 2*X - 1)\n",
    "ax.set_title(\"The rate of change of the function $f(x) = x^2$ at $x=1$ is $f'(1) = 2$.\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But most of the functions we've been working with are functions of multiple variables. The optimization function calculated for a multiple linear regression, for example, presupposes multiple predictors or \"features\".\n",
    "\n",
    "If we wanted to describe the rate of change of such a function, we can't simply say e.g. that the function changes at a particular rate for some particular value of a single variable, say $x_1$, because the function (and its rate of change) by definition also depends on other variables! What we can do instead is to describe how the function changes *with respect to $x_1$*, and the way we do that is *by assuming that we hold the other variables constant*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partial Differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the idea behind **partial differentiation**. Consider the following function of two variables, which should be reminiscent of a multiple linear regression: $f(x_1, x_2) = 3x_1 - 5x_2$. For this function $f$, we could consider how its values change *with respect to $x_1$* or *with respect to $x_2$*. And to calculate these rates of change, we simply apply our familiar rules of differentiation to the relevant variable, *while treating any other variable as a constant*.\n",
    "\n",
    "Thus we can calculate:\n",
    "\n",
    "$\\frac{\\partial f}{\\partial x_1} = 3$\n",
    "\n",
    "$\\frac{\\partial f}{\\partial x_2} = 5$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **gradient** simply collects together *all* the partial derivatives of a function, and in *vector* form. For our function $f$, the first partial derivative tells us that, as we hold $x_2$ fixed, the rate of change of $f$ with respect to $x_1$ is 3. That is, $f$ has a (constant) rate of change of 3 *in the $x_1$-direction*. The second partial derivative tells us that, as we hold $x_1$ fixed, the rate of change of $f$ with respect to $x_2$ is 5. That is, $f$ has a (constant) rate of change of 5 *in the $x_2$-direction*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And so the gradient is defined as follows:\n",
    "\n",
    "$$\\begin{align}\\\\\n",
    "    \\large \\nabla f &= \\sum_i \\dfrac{\\partial f}{\\partial \\theta_i}\\hat{\\theta_i} \\\\\n",
    "            &= \\frac{\\partial f}{\\partial \\theta_1}\\hat{\\theta_1} + \\dots +  \\frac{\\partial f}{\\partial \\theta_n}\\hat{\\theta_n}\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the multivariate case, the gradient tells us how the function is changing **in each dimension**. A large value of the derivative with respect to a particular variable means that the gradient will have a large component in the corresponding direction. Therefore, **the gradient will point in the direction of steepest increase**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the function $z = x^2 + y^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're making a grid of (x, y) points.\n",
    "# Start by making multiple copies of our x vector.\n",
    "\n",
    "x_col = X.reshape(1, 13)\n",
    "x_plane = np.ones(13).reshape(13, 1) @ x_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now do the same for y values.\n",
    "\n",
    "y_col = X.reshape(13, 1)\n",
    "y_plane = y_col @ -np.ones(13).reshape(1, 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = x_plane**2 + y_plane**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot $z$ as a heatmap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "sns.heatmap(z, xticklabels=X, yticklabels=X, annot=True, ax=ax, cmap=\"Blues\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The heatmap shows the values of $z = x^2 + y^2$. Note that the function has a global minimum at $(x, y) = (0, 0)$, indicated in the heatmap by the palest shade of blue. And more generally the shape of the function in $xyz$ space is a bowl or paraboloid ![paraboloid](images/512px-Paraboloid_of_Revolution.svg.png), where the slope of the bowl increases as you get farther away from the vertex. Image source: [wikipedia](https://en.wikipedia.org/wiki/Paraboloid)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In particular, the gradient of $z$ is:\n",
    "\n",
    "$\\large\\nabla z = \\frac{\\partial z}{\\partial x}\\hat{x} + \\frac{\\partial z}{\\partial x}\\hat{y} = 2x\\hat{x} + 2y\\hat{y}$, and so it's clear that, as we move further away from $(0, 0)$ the (absolute value of the) slope increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The observation that the gradient points in the direction of fastest increase is critical to the machine-learning technique of **gradient descent**:\n",
    "\n",
    "First, if the gradient is pointing in the direction of fastest increase, then the *negative* of the gradient will point in the direction of fastest *decrease*. And second, if the function whose slope we're examining is a *cost* function for some modeling algorithm, where the independent variables are the model's parameters, then an adjustment of the values of our parameters in the direction of the negative gradient **will result in a lower value of the cost function** and, thus, a better model! We'll see applications of this idea soon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We turn now to tensors. A **tensor** is a notion from algebra, and it is a generalized notion of a matrix into higher-dimensional spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The utility and relevance of matrices is already apparent. A **matrix** is a two-dimensional array of objects (generally numbers). Matrices of course arise naturally in the context of data that has a row-and-column structure. `NumPy` has a *matrix* class reserved for such two-dimensional objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = np.matrix([[1, 2, 3], [4, 5, 6]])\n",
    "matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `NumPy` Arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But typically we'll be making use of `NumPy`'s *array* class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some [subtle differences](https://stackoverflow.com/questions/4151128/what-are-the-differences-between-numpy-arrays-and-matrices-which-one-should-i-u) between these two, but one key difference is that arrays can have *any* dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The type indicates here that this object is an *n*-dimensional array, where *n* can be any counting number we like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor Uses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Uses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A matrix is a convenient way of representing a certain kind of *structure*. Consider a typical data table, where the columns represent features and the rows represent observations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th></th>\n",
    "        <th> feature1 </th>\n",
    "        <th> feature2 </th>\n",
    "        <th> feature3 </th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th> observation1 </th>\n",
    "        <td> obs1feat1 </td>\n",
    "        <td> obs1feat2 </td>\n",
    "        <td> obs1feat3 </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th> observation2 </th>\n",
    "        <td> obs2feat1 </td>\n",
    "        <td> obs2feat2 </td>\n",
    "        <td> obs2feat3 </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th> observation3 </th>\n",
    "        <td> obs3feat1 </td>\n",
    "        <td> obs3feat2 </td>\n",
    "        <td> obs3feat3 </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th> observation4 </th>\n",
    "        <td> obs4feat1 </td>\n",
    "        <td> obs4feat2 </td>\n",
    "        <td> obs4feat3 </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th> observation5 </th>\n",
    "        <td> obs5feat1 </td>\n",
    "        <td> obs5feat2 </td>\n",
    "        <td> obs5feat3 </td>\n",
    "    </tr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sort of structure is useful for all sorts of data. When do we want or need higher dimensions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uses for Higher-Dimensional Objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One very common use for tensors is in representing color images. To digitize an image, we'll chop it into little bits or *pixels*: The pixel in the top left will represent the extreme top-left of the image, the pixel in the top right will represent the extreme top-right of the image, etc.\n",
    "\n",
    "But now *how* will each pixel represent a part of the image? The chief thing to capture is the *color* of the relevant part of the image (larger patterns of the image will emerge when we take the pixels in aggregate), and we can use our three color channels (red, green, blue) to represent any color we like. But that means that we need to associate each pixel with a *triple* of numbers. We can represent that with a tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the following image:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![paint](images/paint.jpeg) image source: depositphotos.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read it in with `MatPlotLib`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paint_digitized = plt.imread(\"images/paint.jpeg\")\n",
    "paint_digitized.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should see high green numbers on the left (low second-coordinate values), high red numbers in the middle (middling second-coordinate values), and high blue numbers on the right (high second-coordinate values). Let's see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# green\n",
    "paint_digitized[10, 100, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# red\n",
    "paint_digitized[10, 600, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# blue\n",
    "paint_digitized[10, 800, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`PyPlot` can also turn a digitization into an image, so we should be able to reproduce the image by using the `.imshow()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(paint_digitized);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another use for a tensor would be to represent the gradient we constructed above for the function $z = x^2 + y^2$! We could take our grid of $(x, y)$ points and associate each gridpoint with *both* the value of $\\frac{\\partial z}{\\partial x}$ and the value of $\\frac{\\partial z}{\\partial y}$ at that point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_tensor = np.stack((2*x_plane, 2*y_plane))\n",
    "print(grad_tensor.shape)\n",
    "grad_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now each point in our grid is associated with a *double* of values $(\\frac{\\partial z}{\\partial x}, \\frac{\\partial z}{\\partial y})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `np.reshape()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes bits of our data pipeline will expect data to have a certain shape, and it may therefore be necessary to transform data arrays to fit those predetermined structures. `NumPy`'s `reshape()` function will be invaluable for that purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grad_tensor as a row vector\n",
    "grad_tensor.reshape(1, 338)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grad_tensor as a column vector\n",
    "grad_tensor.reshape(338, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grad_tensor as a five-dimensional object\n",
    "grad_tensor.reshape(13, 13, 1, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
