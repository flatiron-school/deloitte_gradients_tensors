{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradients and Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a certain amount of mathematics we'll need before proceeding. This lecture will introduce you to two mathematical notions that are central to machine learning: **gradients** and **tensors**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient is a notion from calculus, and it is a generalized notion of a derivative into higher-dimensional spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivatives in One Dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you first learn calculus, you generally start with *differential* calculus, and you always start with functions of a single variable. Thus you learn derivative shortcut rules such as:\n",
    "\n",
    "$\\frac{d}{dx}[x^2] = 2x$\n",
    "\n",
    "$\\frac{d}{dx}[sin(x)] = cos(x)$\n",
    "\n",
    "$\\frac{d}{dx}[5^x] = 5^xln(5)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there is the Newtonian notation as well:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f'(x) = 2x$ for $f(x) = x^2$\n",
    "\n",
    "$f'(x) = cos(x)$ for $f(x) = sin(x)$\n",
    "\n",
    "$f'(x) = 5^xln(5)$ for $f(x) = 5^x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivatives of Functions of Multiple Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind that a derivative tells us the *rate of change* of a function for any value of its independent variables. To say e.g. that $2x$ is the derivative of $x^2$ is to say that, for any value of $x$, the rate of change, or *slope*, of the function $x^2$ has a value of $2x$. At $x=0$, the slope is $2\\times 0 = 0$; at $x=1$, the slope is $2\\times 1 = 2$; etc.\n",
    "\n",
    "But most of the functions we've been working with are functions of multiple variables. The optimization function calculated for a multiple linear regression, for example, presupposes multiple predictors or \"features\".\n",
    "\n",
    "If we wanted to describe the rate of change of such a function, we can't simply say e.g. that the function changes at a particular rate for some particular value of a single variable, say $x_1$, because the function (and its rate of change) by definition also depends on other variables! What we can do instead is to describe how the function changes *with respect to $x_1$*, and the way we do that is *by assuming that we hold the other variables constant*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partial Differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the idea behind **partial differentiation**. Consider the following function of two variables, which should be reminiscent of a multiple linear regression: $f(x_1, x_2) = 3x_1 - 5x_2$. For this function $f$, we could consider how its values change *with respect to $x_1$* or *with respect to $x_2$*. And to calculate these rates of change, we simply apply our familiar rules of differentiation to the relevant variable, *while treating any other variable as a constant*.\n",
    "\n",
    "Thus we can calculate:\n",
    "\n",
    "$\\frac{\\partial f}{\\partial x_1} = 3$\n",
    "\n",
    "$\\frac{\\partial f}{\\partial x_2} = 5$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **gradient** simply collects together *all* the partial derivatives of a function, and in *vector* form. For our function $f$, the first partial derivative tells us that, as we hold $x_2$ fixed, the rate of change of $f$ with respect to $x_1$ is 3. That is, $f$ has a (constant) rate of change of 3 *in the $x_1$-direction*. The second partial derivative tells us that, as we hold $x_1$ fixed, the rate of change of $f$ with respect to $x_2$ is 5. That is, $f$ has a (constant) rate of change of 5 *in the $x_2$-direction*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And so the gradient is defined as follows:\n",
    "\n",
    "$$\\begin{align}\\\\\n",
    "    \\large \\nabla ff &= \\sum_i \\dfrac{\\partial f}{\\partial \\theta_i}\\hat{\\theta_i} \\\\\n",
    "            &= \\frac{\\partial f}{\\partial \\theta_1}\\hat{\\theta_1} + \\dots +  \\frac{\\partial f}{\\partial \\theta_n}\\hat{\\theta_n}\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the multivariate case, the gradient tells us how the function is changing **in each dimension**. A large value of the derivative with respect to a particular variable means that the gradient will have a large component in the corresponding direction. Therefore, **the gradient will point in the direction of steepest increase**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
